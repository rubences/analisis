{"cells":[{"cell_type":"markdown","metadata":{"id":"kJBT2qy6xv5y"},"source":["# Caso Práctico de Módulo 2: Clasificación de Tweets en Español (Análisis de Sentimientos)\n","\n","* En el siguiente ejercicio tiene como objetivo aplicar los conocimientos vistos hasta el momento de:\n","    - Normalización de textos (procesamiento de textos)\n","    - Clasificación de textos\n","    \n","    \n","* El ejercicio consiste en clasificar una serie de tweets en Español que estan clasificados como '***positivos***', '***neutros***', '***negativos***' o '***None***' (desconocido), aunque solo vamos a trabajar con los tweets clasificados correctamente (los no desconocidos).\n","\n","\n","* Para este ejercicio se pide realizar todo el proceso de clasificación visto hasta el momento:\n","    1. Carga de los datos (ya implementado en el ejercicio)\n","    2. Normalización de los tweets\n","    3. Creacción de la Bolsa de Palabras de frecuencias\n","    4. Particionado de Datos\n","    5. Creacción de modelos\n","    6. Evaluación de los modelos\n","\n"]},{"cell_type":"markdown","metadata":{"id":"H1Mt11M6xv51"},"source":["<hr>\n","\n","\n","## Carga de Datos\n","\n","\n","* El primer paso que vamos a realizar es el de cargar los datos.\n","\n","\n","* Estos datos estan en un archivo con extensión '*txt*' pero tienen estructura de '*csv*' y tienen como separador '***::::***'.\n","\n","\n","* Este fichero lo podemos leer como un '*csv*' con pandas, estructurándolo de la siguiente manera:\n","    - **Posición 0**: Tweet\n","    - **Posición 1**: Sentimiento (Positivo | Neutro | Negativo)\n","    \n","    \n","* Los tweets estan clasificados con 4 etiquetas, pero vamos a trabajar solo con los tweets que sean '***positivos***', '***neutros***' o '***negativos***'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5PbX_BGoxv52","outputId":"f9add892-d312-4f3a-a08d-15e11ceb2252"},"outputs":[{"name":"stdout","output_type":"stream","text":["Número de Tweets Cargados: 5735\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet</th>\n","      <th>sentimiento</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n","      <td>neutro</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>@marodriguezb Gracias MAR</td>\n","      <td>positivo</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Off pensando en el regalito Sinde, la que se v...</td>\n","      <td>negativo</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n","      <td>positivo</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Toca @crackoviadeTV3 . Grabación dl especial N...</td>\n","      <td>positivo</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               tweet sentimiento\n","1  @PauladeLasHeras No te libraras de ayudar me/n...      neutro\n","2                          @marodriguezb Gracias MAR    positivo\n","3  Off pensando en el regalito Sinde, la que se v...    negativo\n","4  Conozco a alguien q es adicto al drama! Ja ja ...    positivo\n","6  Toca @crackoviadeTV3 . Grabación dl especial N...    positivo"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","tweets_file = '../data/tweets_castellano.txt'\n","df = pd.read_csv(tweets_file, sep=\"::::\", names=['tweet','sentimiento'] ,engine='python')\n","df = df[df['sentimiento'].isin(['positivo', 'neutro', 'negativo'])]\n","tweets = [tuple(x) for x in df.values]\n","print('Número de Tweets Cargados: {num}'.format(num=len(tweets)))\n","df.head(5)"]},{"cell_type":"markdown","metadata":{"id":"eq1b_Xuuxv53"},"source":["<hr>\n","\n","\n","## Normalización\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"gY1O6FxTxv53","executionInfo":{"status":"error","timestamp":1700048359318,"user_tz":-60,"elapsed":3251,"user":{"displayName":"Rubén Juárez Cádiz","userId":"13827813516298494191"}},"outputId":"23d3a7c8-2b1f-4e7f-e73d-472cc5d13b30"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-d4a20bf909b4>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'normalized_tweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize_tweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}],"source":["import re\n","from nltk.corpus import stopwords\n","from nltk.stem import SnowballStemmer\n","import nltk\n","nltk.download('stopwords')\n","\n","def normalize_tweet(tweet):\n","    tweet = tweet.lower()\n","    tweet = re.sub(r'\\W+', ' ', tweet)\n","    tweet = tweet.split()\n","    tweet = [word for word in tweet if word not in stopwords.words('spanish')]\n","    stemmer = SnowballStemmer('spanish')\n","    tweet = [stemmer.stem(word) for word in tweet]\n","    return ' '.join(tweet)\n","\n","df['normalized_tweet'] = df['tweet'].apply(normalize_tweet)\n"]},{"cell_type":"markdown","metadata":{"id":"HYKVL2K5xv54"},"source":["<hr>\n","\n","\n","## Bolsa de Palabras de Frecuencias\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":210},"id":"RNEVmfbSxv54","executionInfo":{"status":"error","timestamp":1700048379257,"user_tz":-60,"elapsed":204,"user":{"displayName":"Rubén Juárez Cádiz","userId":"13827813516298494191"}},"outputId":"280621d6-2e1c-4463-f172-11d41785633c"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-c4ff4a05da69>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'normalized_tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(df['normalized_tweet'])\n"]},{"cell_type":"markdown","metadata":{"id":"wj1xSn7Nxv54"},"source":["<hr>\n","\n","\n","## Particionado de Datos (Train y Test)\n","\n","* Particionar los datos en conjunto de Train y Test de la siguiente manera:\n","    - 80% de datos de entrenamiento\n","    - 20% de datos de test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":227},"id":"tQA-J4iYxv54","executionInfo":{"status":"error","timestamp":1700048389347,"user_tz":-60,"elapsed":207,"user":{"displayName":"Rubén Juárez Cádiz","userId":"13827813516298494191"}},"outputId":"ecbabd1a-64cf-486e-c73d-64a754f20862"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-8382d3818560>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentimiento'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","y = df['sentimiento']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"]},{"cell_type":"markdown","metadata":{"id":"HpbfTy_gxv55"},"source":["<hr>\n","\n","\n","## Creacción del Modelo\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":210},"id":"o7JyeVB5xv55","executionInfo":{"status":"error","timestamp":1700048400129,"user_tz":-60,"elapsed":316,"user":{"displayName":"Rubén Juárez Cádiz","userId":"13827813516298494191"}},"outputId":"dcc8c810-925d-4601-f28b-5e0e39d919fa"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-fa22823560d3>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"]}],"source":["from sklearn.naive_bayes import MultinomialNB\n","\n","model = MultinomialNB()\n","model.fit(X_train, y_train)\n"]},{"cell_type":"markdown","metadata":{"id":"CBiybttvxv55"},"source":["<hr>\n","\n","\n","## Evaluación del Modelo\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"7cz_VQqqxv55","executionInfo":{"status":"error","timestamp":1700048410438,"user_tz":-60,"elapsed":320,"user":{"displayName":"Rubén Juárez Cádiz","userId":"13827813516298494191"}},"outputId":"faa0410c-17e8-4113-b7b4-f5a35105b197"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-43c72e10f7d3>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"]}],"source":["from sklearn.metrics import classification_report, confusion_matrix\n","\n","y_pred = model.predict(X_test)\n","print(classification_report(y_test, y_pred))\n","print(confusion_matrix(y_test, y_pred))\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}